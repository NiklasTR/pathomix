import os
import pandas as pd
import shutil
import random
import numpy as np
import time
import multiprocessing
from keras.preprocessing.image import ImageDataGenerator
from keras import optimizers
from keras.models import Model
from keras.layers import Dense, GlobalAveragePooling2D, Dropout
from keras.utils import Sequence, to_categorical
from sklearn.model_selection import StratifiedKFold
import efficientnet.keras as efn
from skimage.transform import resize
import imageio

import wandb
from wandb.keras import WandbCallback


def create_data_frame(base_dir):
    data_list = []
    for root, dirs, files in os.walk(base_dir):
        for file in files:
            if file.endswith('.png'):
                dir_name = os.path.basename(root)
                relative_path = os.path.join('.', dir_name, file)
                data_list.append({"relative_path": relative_path, "label": os.path.basename(root)})
    data_frame = pd.DataFrame(data_list)
    data_frame.loc[:, ('num_label')] = data_frame.label.astype('category').cat.codes
    return data_frame


def split_data_frame(df, train_idx, val_idx):
    df_train = df.iloc[train_idx]
    df_val = df.iloc[val_idx]
    return df_train, df_val


def list_all_files_for_class(base_dir, label):
    list_of_files = []
    for root, dirs, files in os.walk(base_dir):
        for file in files:
            if not label in root:
                break
            if file.endswith('.png'):
                dir_name = os.path.basename(root)
                list_of_files.append(os.path.join(dir_name, file))
    return list_of_files


def pick_random_sample(file_list, proportion=0.2):
    pick_n = int(len(file_list) * proportion)
    return random.sample(file_list, pick_n)


def move_files(file_list, source_dir, target_dir, label):
    target_folder = os.path.join(target_dir, label)
    if not os.path.exists(target_folder):
        os.mkdir(target_folder)
    for f in file_list:
        source_path = os.path.join(source_dir, f)
        target_path = os.path.join(target_dir, f)
        shutil.move(source_path, target_path)


def create_test_set(base_dir=os.path.join(os.environ['PATHOMIX_DATA'], 'Jakob_cancer_detection', 'train')):
    labels = ['ADIMUC', 'STRMUS', 'TUMSTU']
    source_dir = '/home/pmf/Documents/DataMining/datasets/pathology/Jakob_cancer_detection/train'
    target_dir = '/home/pmf/Documents/DataMining/datasets/pathology/Jakob_cancer_detection/test'

    for l in labels:
        total_list = list_all_files_for_class(base_dir, l)
        random_list = pick_random_sample(total_list, proportion=0.2)
        move_files(random_list, source_dir, target_dir, l)

def random_crop(img, random_crop_size=(456,456)):
    # Note: image_data_format is 'channel_last'
    assert img.shape[2] == 3
    height, width = img.shape[0], img.shape[1]
    dy, dx = random_crop_size
    x = np.random.randint(0, width - dx + 1)
    y = np.random.randint(0, height - dy + 1)
    return img[y:(y+dy), x:(x+dx), :]


def crop_generator(batches, crop_length):
    """Take as input a Keras ImageGen (Iterator) and generate random
    crops from the image batches generated by the original iterator.
    """
    while True:
        batch_x, batch_y = next(batches)
        batch_crops = np.zeros((batch_x.shape[0], crop_length, crop_length, 3))
        for i in range(batch_x.shape[0]):
            batch_crops[i] = random_crop(batch_x[i], (crop_length, crop_length))
        yield (batch_crops, batch_y)



class DataLoader(Sequence):

    def __init__(self, data_frame, batch_size, data_dir, dim=(456,456), n_channels=3, shuffle=True):
        self.data_frame = data_frame
        self.list_IDs = list(self.data_frame.index)
        self.labels = to_categorical(self.data_frame['num_label'])
        self.n_classes = len(self.data_frame['label'].unique())
        self.dim = dim
        self.n_channels = n_channels
        self.batch_size = batch_size
        self.shuffle = shuffle
        self.data_dir = data_dir
        self.load_data_set()
        self.on_epoch_end()

    def load_data_set(self):
        data_list = []
        for fp in self.data_frame['relative_path']:
            file_path = os.path.join(self.data_dir, fp)
            # cut to relevant size
            file = imageio.imread(file_path)[:self.dim[0],:self.dim[1]]
            data_list.append(file)
        self.data = np.array(data_list)


    def on_epoch_end(self):
        'Updates indexes after each epoch'
        self.indexes = np.arange(len(self.list_IDs))
        if self.shuffle == True:
            np.random.shuffle(self.indexes)

    def __len__(self):
        return int(np.ceil(len(self.list_IDs) / float(self.batch_size)))

    def __getitem__(self, index):
        'Generate one batch of data'
        # Generate indexes of the batch
        indexes = self.indexes[index * self.batch_size:(index + 1) * self.batch_size]

        # Generate data
        X, y = self.__data_generation(indexes)

        return X, y

    def __data_generation(self, indexes):
        'Generates data containing batch_size samples'  # X : (n_samples, *dim, n_channels)
        # Initialization
        X = np.empty((self.batch_size, *self.dim, self.n_channels))
        y = np.empty((self.batch_size, self.n_classes), dtype=int)

        # Generate data
        for idx, ID in enumerate(indexes):
            # Store sample
            X[idx,] = self.data[ID]

            # Store class
            y[idx,] = self.labels[ID]

        return X, y


optimizing_parameters = dict(
    lr=0.1,
    decay=1e-6,
    momentum=0.9
)

timestr = time.strftime("%Y_%m_%d-%H:%M:%S")
debug = False
if debug:
    wandb.init(name=timestr, config=optimizing_parameters, project="first_aws")
else:
    wandb.init(name=timestr, config=optimizing_parameters, project="td_no_norm")

if __name__ == '__main__':
    '''
    parser = argparse.ArgumentParser(description='Give parameters for tumor detection fine tuning')
    parser.add_argument("--learning-rate", help="")
    parser.add_argument("--decay", help="")
    parser.add_argument("--momentum", help="")

    args = parser.parse_args()
    lr = args.lr
    decay = args.decay
    momentum = args.momentum
    '''

    base_dir = os.path.join(os.environ['PATHOMIX_DATA'], 'Jakob_cancer_detection')
    data_dir = os.path.join(base_dir, 'train')
    vis_dir = os.path.join(base_dir, 'visualize')

    data_gen_dict = dict(
        featurewise_center=False,
        samplewise_center=False,
        featurewise_std_normalization=False,
        samplewise_std_normalization=False,
        rotation_range_train=0,
        width_shift_range_train=0,
        height_shift_range_train=0,
        horizontal_flip_train=True,
        vertical_flip_train=True,
        fill_mode_train='constant',
        cval_train=0,
        rotation_range_val=0,
        width_shift_range_val=0,
        height_shift_range_val=0,
        horizontal_flip_val=False,
        vertical_flip_val=False,
        fill_mode_val='constant',
        cval_val=0,
        class_mode='categorical',
        x_col='relative_path',
        y_col='label',
        do_augmentation=True,
    )

    if debug:
        hyperparameter_dict = dict(
            seed=42,
            batch_size=8,
            input_size=(224, 224),
            crop_length=None
        )
    else:
        hyperparameter_dict = dict(
            seed=42,
            batch_size=64,
            input_size=(456, 456),
            crop_length=456
        )

    df_total = create_data_frame(base_dir=data_dir)
    kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=hyperparameter_dict["seed"])
    # get indices for train and validation
    train_idx, val_idx = next(kf.split(X=np.zeros(len(df_total)), y=df_total['label']))
    df_train, df_val = split_data_frame(df_total, train_idx, val_idx)

    if data_gen_dict['do_augmentation']:
        print('create data generators')
        train_datagen = ImageDataGenerator(
            featurewise_center=data_gen_dict["featurewise_center"],
            samplewise_center=data_gen_dict["samplewise_center"],
            featurewise_std_normalization=data_gen_dict["featurewise_std_normalization"],
            samplewise_std_normalization=data_gen_dict["samplewise_std_normalization"],
            rotation_range=data_gen_dict["rotation_range_train"],
            width_shift_range=data_gen_dict["width_shift_range_train"],
            height_shift_range=data_gen_dict["height_shift_range_train"],
            horizontal_flip=data_gen_dict["horizontal_flip_train"],
            vertical_flip=data_gen_dict["vertical_flip_train"],
            fill_mode=data_gen_dict["fill_mode_train"],
            cval=data_gen_dict["cval_train"]
        )
        val_datagen = ImageDataGenerator(
            featurewise_center=data_gen_dict["featurewise_center"],
            samplewise_center=data_gen_dict["samplewise_center"],
            featurewise_std_normalization=data_gen_dict["featurewise_std_normalization"],
            samplewise_std_normalization=data_gen_dict["samplewise_std_normalization"],
            rotation_range=data_gen_dict["rotation_range_val"],
            width_shift_range=data_gen_dict["width_shift_range_val"],
            height_shift_range=data_gen_dict["height_shift_range_val"],
            horizontal_flip=data_gen_dict["horizontal_flip_val"],
            vertical_flip=data_gen_dict["vertical_flip_val"],
            fill_mode=data_gen_dict["fill_mode_val"],
            cval=data_gen_dict["cval_val"],
            preprocessing_function=random_crop
        )
        #train_datagen.standardize()

        print('create training batch generators')
        train_generator = train_datagen.flow_from_dataframe(df_train, data_dir,
                                                            x_col=data_gen_dict["x_col"],
                                                            y_col=data_gen_dict["y_col"],
                                                            weight_col=None,
                                                            target_size=hyperparameter_dict["input_size"],
                                                            class_mode=data_gen_dict["class_mode"],
                                                            batch_size=hyperparameter_dict["batch_size"],
                                                            shuffle=True,
                                                            seed=hyperparameter_dict["seed"],
                                                            save_to_dir=None,
                                                            save_prefix="aug_test_")

        val_generator = val_datagen.flow_from_dataframe(df_val, data_dir,
                                                        x_col=data_gen_dict["x_col"],
                                                        y_col=data_gen_dict["y_col"],
                                                        weight_col=None,
                                                        target_size=hyperparameter_dict["input_size"],
                                                        class_mode=data_gen_dict["class_mode"],
                                                        batch_size=hyperparameter_dict["batch_size"],
                                                        shuffle=True,
                                                        seed=hyperparameter_dict["seed"],
                                                        save_to_dir=None,
                                                        save_prefix="aug_test_val")

        # for hyperparameter tracking
        devide_by = 5
        labels = list(train_generator.class_indices.keys())
        step_per_epoch = len(train_generator) // devide_by
        validation_steps = len(val_generator)
    else:
        devide_by = 5
        labels= list(df_total['label'].unique())
        params = {
            'batch_size': hyperparameter_dict["batch_size"],
            'data_dir': data_dir,
            'dim': hyperparameter_dict['input_size'],
            'n_channels': 3,
            'shuffle': True
        }
        train_generator = DataLoader(data_frame=df_train, **params)
        step_per_epoch = train_generator.__len__()//devide_by
        val_generator = DataLoader(data_frame=df_val, **params)
        validation_steps = val_generator.__len__()
    hp_dict = dict(
        seed=hyperparameter_dict["seed"],
        batch_size=hyperparameter_dict["batch_size"],
        input_size=hyperparameter_dict["input_size"],
        epochs=10,
        nesterov=False,
        labels=labels,
        step_per_epoch=step_per_epoch,
        verbose=2,
        validation_steps=validation_steps,
        validation_freq=1,
        class_weight=None,
        max_queue_size=multiprocessing.cpu_count()*3,
        workers=multiprocessing.cpu_count(),
        use_multiprocessing=False,
        shuffle=True,
        initial_epoch=0,
        loss='categorical_crossentropy',
        metriccs=['categorical_accuracy'],
        optimizer='rmsprop'
    )

    print("load model")
    # load model with pretrained- weights
    if debug:
        model = efn.EfficientNetB0(weights='imagenet', include_top=False)
    else:
        model = efn.EfficientNetB5(weights='imagenet', include_top=False)

    # freeze all layers in pretrained model
    for l in model.layers:
        l.trainable = False

    x = model.output
    x = GlobalAveragePooling2D()(x)
    x = Dropout(rate=0.2)(x)
    pred = Dense(len(hp_dict["labels"]), activation='softmax')(x)

    my_model = Model(inputs=model.input, outputs=pred)

    print('complile model')
    #model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['accuracy'])
    if hp_dict['optimizer'] == 'sgd':
        sgd = optimizers.SGD(learning_rate=optimizing_parameters["lr"], momentum=optimizing_parameters["momentum"],
                            nesterov=hp_dict["nesterov"], decay=optimizing_parameters["decay"])
        my_model.compile(optimizer=sgd, loss=hp_dict["loss"], metrics=hp_dict["metriccs"])
    elif hp_dict['optimizer'] == 'rmsprop':
        my_model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=hp_dict["metriccs"])
    else:
        print("invalid optimizer")



    '''
    wandb_callback = WandbCallback(monitor='val_loss', mode='max', save_weights_only=False, log_weights=False,
                                   log_gradients=False, save_model=False, training_data=None,
                                   validation_data=None, labels=hp_dict["labels"], data_type="image", predictions=32,
                                   generator=val_generator)
    '''
    wandb_callback = WandbCallback(monitor='val_loss', mode='max', save_weights_only=False, log_weights=False,
                                   log_gradients=False, save_model=False, training_data=None,
                                   validation_data=None, labels=None, data_type=None, predictions=0,
                                   generator=val_generator)

    all_paras_dict_determined = {**data_gen_dict, **hp_dict}
    #all_params_dict = {**all_paras_dict_determined, **optimizing_parameters}
    wandb.config.update(params=all_paras_dict_determined)

    # wand_callbacks = WandbCallback(data_type="image", labels=labels)
    print('start training')
    if data_gen_dict['do_augmentation']:
        results = my_model.fit_generator(train_generator, steps_per_epoch=hp_dict["step_per_epoch"],
                                         epochs=hp_dict["epochs"], verbose=hp_dict["verbose"], callbacks=[wandb_callback],
                                         validation_data=val_generator, validation_steps=hp_dict["validation_steps"],
                                         validation_freq=hp_dict["validation_freq"], class_weight=hp_dict["class_weight"],
                                         max_queue_size=hp_dict["max_queue_size"], workers=hp_dict["workers"],
                                         use_multiprocessing=hp_dict["use_multiprocessing"], shuffle=hp_dict["shuffle"],
                                         initial_epoch=hp_dict["initial_epoch"])  # (x=train_generator, epochs=callbacks=[WandbCallback()])
    else:
        my_model.fit(train_generator.data, to_categorical(train_generator.labels),
                  batch_size=hyperparameter_dict["batch_size"],
                  epochs=hp_dict["epochs"],
                  validation_data=(val_generator.data, to_categorical(val_generator.labels)),
                  shuffle=True)
    print(results.params)

    #wandb.log({"val_loss": results.params["val_accuracy"]})

    '''
    for epoch in range(1, wandb.config.epochs + 1):
    print(epoch)
    for batch_idx in range(20): #range(len(train_generator)):
        x, y = train_generator.next()
        train_loss = model.train_on_batch(x, y, sample_weight=None, class_weight=None, reset_metrics=True)
        print(train_loss)
    '''
