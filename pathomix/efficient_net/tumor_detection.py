import os
import pandas as pd
import shutil
import random
import numpy as np
import time
import multiprocessing
from keras.preprocessing.image import ImageDataGenerator
from keras import optimizers
from keras.models import Model
from keras.layers import Dense
from sklearn.model_selection import StratifiedKFold
import efficientnet.keras as efn

import wandb
from wandb.keras import WandbCallback


def create_data_frame(base_dir):
    data_list = []
    for root, dirs, files in os.walk(base_dir):
        for file in files:
            if file.endswith('.png'):
                dir_name = os.path.basename(root)
                relative_path = os.path.join('.', dir_name, file)
                data_list.append({"relative_path": relative_path, "label": os.path.basename(root)})
    data_frame = pd.DataFrame(data_list)
    return data_frame


def split_data_frame(df, train_idx, val_idx):
    df_train = df.iloc[train_idx]
    df_val = df.iloc[val_idx]
    return df_train, df_val


def list_all_files_for_class(base_dir, label):
    list_of_files = []
    for root, dirs, files in os.walk(base_dir):
        for file in files:
            if not label in root:
                break
            if file.endswith('.png'):
                dir_name = os.path.basename(root)
                list_of_files.append(os.path.join(dir_name, file))
    return list_of_files


def pick_random_sample(file_list, proportion=0.2):
    pick_n = int(len(file_list) * proportion)
    return random.sample(file_list, pick_n)


def move_files(file_list, source_dir, target_dir, label):
    target_folder = os.path.join(target_dir, label)
    if not os.path.exists(target_folder):
        os.mkdir(target_folder)
    for f in file_list:
        source_path = os.path.join(source_dir, f)
        target_path = os.path.join(target_dir, f)
        shutil.move(source_path, target_path)


def create_test_set(base_dir=os.path.join(os.environ['PATHOMIX_DATA'], 'Jakob_cancer_detection', 'train')):
    labels = ['ADIMUC', 'STRMUS', 'TUMSTU']
    source_dir = '/home/pmf/Documents/DataMining/datasets/pathology/Jakob_cancer_detection/train'
    target_dir = '/home/pmf/Documents/DataMining/datasets/pathology/Jakob_cancer_detection/test'

    for l in labels:
        total_list = list_all_files_for_class(base_dir, l)
        random_list = pick_random_sample(total_list, proportion=0.2)
        move_files(random_list, source_dir, target_dir, l)

def random_crop(img, random_crop_size=(456,456)):
    # Note: image_data_format is 'channel_last'
    assert img.shape[2] == 3
    height, width = img.shape[0], img.shape[1]
    dy, dx = random_crop_size
    x = np.random.randint(0, width - dx + 1)
    y = np.random.randint(0, height - dy + 1)
    return img[y:(y+dy), x:(x+dx), :]


def crop_generator(batches, crop_length):
    """Take as input a Keras ImageGen (Iterator) and generate random
    crops from the image batches generated by the original iterator.
    """
    while True:
        batch_x, batch_y = next(batches)
        batch_crops = np.zeros((batch_x.shape[0], crop_length, crop_length, 3))
        for i in range(batch_x.shape[0]):
            batch_crops[i] = random_crop(batch_x[i], (crop_length, crop_length))
        yield (batch_crops, batch_y)


optimizing_parameters = dict(
    lr=0.1,
    decay=1e-6,
    momentum=0.9
)

timestr = time.strftime("%Y_%m_%d-%H:%M:%S")
debug = False
if debug:
    wandb.init(name=timestr, config=optimizing_parameters, project="first_aws")
else:
    wandb.init(name=timestr, config=optimizing_parameters, project="td_no_norm")

if __name__ == '__main__':
    '''
    parser = argparse.ArgumentParser(description='Give parameters for tumor detection fine tuning')
    parser.add_argument("--learning-rate", help="")
    parser.add_argument("--decay", help="")
    parser.add_argument("--momentum", help="")

    args = parser.parse_args()
    lr = args.lr
    decay = args.decay
    momentum = args.momentum
    '''
    base_dir = os.path.join(os.environ['PATHOMIX_DATA'], 'Jakob_cancer_detection')
    data_dir = os.path.join(base_dir, 'train')
    vis_dir = os.path.join(base_dir, 'visualize')

    data_gen_dict = dict(
        featurewise_center=False,
        samplewise_center=False,
        featurewise_std_normalization=False,
        samplewise_std_normalization=False,
        rotation_range_train=0,
        width_shift_range_train=0,
        height_shift_range_train=0,
        horizontal_flip_train=True,
        vertical_flip_train=True,
        fill_mode_train='constant',
        cval_train=0,
        rotation_range_val=0,
        width_shift_range_val=0,
        height_shift_range_val=0,
        horizontal_flip_val=False,
        vertical_flip_val=False,
        fill_mode_val='constant',
        cval_val=0,
        class_mode='categorical',
        x_col='relative_path',
        y_col='label'
    )

    if debug:
        hyperparameter_dict = dict(
            seed=42,
            batch_size=8,
            input_size=(224, 224),
            crop_length=None
        )
    else:
        hyperparameter_dict = dict(
            seed=42,
            batch_size=64,
            input_size=(456, 456),
            crop_length=456
        )


    print('create data generators')
    train_datagen = ImageDataGenerator(
        featurewise_center=data_gen_dict["featurewise_center"],
        samplewise_center=data_gen_dict["samplewise_center"],
        featurewise_std_normalization=data_gen_dict["featurewise_std_normalization"],
        samplewise_std_normalization=data_gen_dict["samplewise_std_normalization"],
        rotation_range=data_gen_dict["rotation_range_train"],
        width_shift_range=data_gen_dict["width_shift_range_train"],
        height_shift_range=data_gen_dict["height_shift_range_train"],
        horizontal_flip=data_gen_dict["horizontal_flip_train"],
        vertical_flip=data_gen_dict["vertical_flip_train"],
        fill_mode=data_gen_dict["fill_mode_train"],
        cval=data_gen_dict["cval_train"]
    )
    val_datagen = ImageDataGenerator(
        featurewise_center=data_gen_dict["featurewise_center"],
        samplewise_center=data_gen_dict["samplewise_center"],
        featurewise_std_normalization=data_gen_dict["featurewise_std_normalization"],
        samplewise_std_normalization=data_gen_dict["samplewise_std_normalization"],
        rotation_range=data_gen_dict["rotation_range_val"],
        width_shift_range=data_gen_dict["width_shift_range_val"],
        height_shift_range=data_gen_dict["height_shift_range_val"],
        horizontal_flip=data_gen_dict["horizontal_flip_val"],
        vertical_flip=data_gen_dict["vertical_flip_val"],
        fill_mode=data_gen_dict["fill_mode_val"],
        cval=data_gen_dict["cval_val"],
        preprocessing_function=random_crop
    )

    df_total = create_data_frame(base_dir=data_dir)
    kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=hyperparameter_dict["seed"])
    # get indices for train and validation
    train_idx, val_idx = next(kf.split(X=np.zeros(len(df_total)), y=df_total['label']))
    df_train, df_val = split_data_frame(df_total, train_idx, val_idx)

    print('create training batch generators')
    train_generator = train_datagen.flow_from_dataframe(df_train, data_dir,
                                                        x_col=data_gen_dict["x_col"],
                                                        y_col=data_gen_dict["y_col"],
                                                        weight_col=None,
                                                        target_size=hyperparameter_dict["input_size"],
                                                        class_mode=data_gen_dict["class_mode"],
                                                        batch_size=hyperparameter_dict["batch_size"],
                                                        shuffle=True,
                                                        seed=hyperparameter_dict["seed"],
                                                        save_to_dir=None,
                                                        save_prefix="aug_test_")

    val_generator = val_datagen.flow_from_dataframe(df_val, data_dir,
                                                    x_col=data_gen_dict["x_col"],
                                                    y_col=data_gen_dict["y_col"],
                                                    weight_col=None,
                                                    target_size=hyperparameter_dict["input_size"],
                                                    class_mode=data_gen_dict["class_mode"],
                                                    batch_size=hyperparameter_dict["batch_size"],
                                                    shuffle=True,
                                                    seed=hyperparameter_dict["seed"],
                                                    save_to_dir=None,
                                                    save_prefix="aug_test_val")



    devide_by = 5
    hp_dict = dict(
        seed=hyperparameter_dict["seed"],
        batch_size=hyperparameter_dict["batch_size"],
        input_size=hyperparameter_dict["input_size"],
        epochs=10,
        nesterov=False,
        labels=list(train_generator.class_indices.keys()),
        step_per_epoch=len(train_generator)//devide_by,
        verbose=2,
        validation_steps=len(val_generator),
        validation_freq=1,
        class_weight=None,
        max_queue_size=multiprocessing.cpu_count()*3,
        workers=multiprocessing.cpu_count(),
        use_multiprocessing=False,
        shuffle=True,
        initial_epoch=0
    )

    print("load model")
    # load model with pretrained- weights
    if debug:
        model = efn.EfficientNetB0(weights='imagenet')
    else:
        model = efn.EfficientNetB5(weights='imagenet')
    model.layers.pop()

    # freeze all layers in pretrained model
    for l in model.layers:
        l.trainable = False

    x = model.output
    pred = Dense(len(hp_dict["labels"]), activation='softmax')(x)

    my_model = Model(inputs=model.input, outputs=pred)

    sgd = optimizers.SGD(learning_rate=optimizing_parameters["lr"], momentum=optimizing_parameters["momentum"],
                         nesterov=hp_dict["nesterov"], decay=optimizing_parameters["decay"])
    print('complile model')
    #model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['accuracy'])
    my_model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['categorical_accuracy'])



    '''
    wandb_callback = WandbCallback(monitor='val_loss', mode='max', save_weights_only=False, log_weights=False,
                                   log_gradients=False, save_model=False, training_data=None,
                                   validation_data=None, labels=hp_dict["labels"], data_type="image", predictions=32,
                                   generator=val_generator)
    '''
    wandb_callback = WandbCallback(monitor='val_loss', mode='max', save_weights_only=False, log_weights=False,
                                   log_gradients=False, save_model=False, training_data=None,
                                   validation_data=None, labels=None, data_type=None, predictions=0,
                                   generator=val_generator)

    all_paras_dict_determined = {**data_gen_dict, **hp_dict}
    #all_params_dict = {**all_paras_dict_determined, **optimizing_parameters}
    wandb.config.update(params=all_paras_dict_determined)

    # wand_callbacks = WandbCallback(data_type="image", labels=labels)
    print('start training')
    results = my_model.fit_generator(train_generator, steps_per_epoch=hp_dict["step_per_epoch"],
                                     epochs=hp_dict["epochs"], verbose=hp_dict["verbose"], callbacks=[wandb_callback],
                                     validation_data=val_generator, validation_steps=hp_dict["validation_steps"],
                                     validation_freq=hp_dict["validation_freq"], class_weight=hp_dict["class_weight"],
                                     max_queue_size=hp_dict["max_queue_size"], workers=hp_dict["workers"],
                                     use_multiprocessing=hp_dict["use_multiprocessing"], shuffle=hp_dict["shuffle"],
                                     initial_epoch=hp_dict["initial_epoch"])  # (x=train_generator, epochs=callbacks=[WandbCallback()])
    print(results.params)

    #wandb.log({"val_loss": results.params["val_accuracy"]})

    '''
    for epoch in range(1, wandb.config.epochs + 1):
    print(epoch)
    for batch_idx in range(20): #range(len(train_generator)):
        x, y = train_generator.next()
        train_loss = model.train_on_batch(x, y, sample_weight=None, class_weight=None, reset_metrics=True)
        print(train_loss)
    '''
